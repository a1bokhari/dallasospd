- copy:
    content: '#!/bin/bash

      /var/lib/container-config-scripts/pacemaker_mutex_restart_bundle.sh --lock $*
      2>&1 | logger -t certmonger'
    dest: /usr/bin/certmonger-ha-resource-refresh.sh
    mode: '0700'
    setype: certmonger_unconfined_exec_t
  name: create certificate rotation script for HA services
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/collectd
    setype: svirt_sandbox_file_t
- include_role:
    name: tripleo_provision_mcelog
  name: import provision_mcelog
  when: false
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/ceilometer
    setype: svirt_sandbox_file_t
- name: enable virt_sandbox_use_netlink for healthcheck
  seboolean:
    name: virt_sandbox_use_netlink
    persistent: true
    state: true
- block:
  - name: register common env file
    register: p
    stat:
      path: /etc/contrail/common_contrail.env
  - copy:
      content: ''
      dest: /etc/contrail/common_contrail.env
    name: create common env file
    when: not p.stat.exists
  - name: set contrail base calculated parameters
    set_fact:
      contrail_base_env_params:
        AAA_MODE: cloud-admin
        CLOUD_ORCHESTRATOR: openstack
        OPENSTACK_VERSION: train
        REDIS_SERVER_PORT: 6379
  - lineinfile:
      dest: /etc/contrail/common_contrail.env
      line: '{{ item.key }}={{ item.value }}'
      regexp: ^{{ item.key }}.*
      state: present
    name: write contrail base parameters to environment file
    with_dict: '{{ contrail_base_env_params }}'
  - name: get external contrail config node ips
    register: contrail_config_ips
    shell: hiera -c /etc/puppet/hiera.yaml contrail_config_ips
  - name: turn contrail config ips into string
    set_fact:
      contrail_config_ips_arr: '{{ contrail_config_ips.stdout | regex_replace(''nil'',
        ''[]'') | to_json | from_json }}'
  - set_fact:
      contrail_config_ips_string: '{{ contrail_config_ips_string | default('''')  +
        '' '' + item }}'
    with_items: '{{ contrail_config_ips_arr }}'
  - set_fact:
      contrail_config_ips_string: '{{ contrail_config_ips_string | default('''') |
        trim | regex_replace('' '', '','') }}'
  - name: contrail operator node ips (names) param name
    set_fact:
      contrail_nodes_param_name: contrail_operator_node_ips
  - name: get contrail operator node ips
    register: contrail_operator_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
  - name: turn contrail operator ips into string
    set_fact:
      contrail_operator_nodes_string: '{{ contrail_operator_nodes_list.stdout | regex_replace(''nil'',
        ''[]'') | from_json | join('','') }}'
  - name: set contrail config ips
    set_fact:
      contrail_config_nodes_string: '{{ contrail_config_ips_string }}'
  - block:
    - name: config api node ips (names) param name
      set_fact:
        contrail_nodes_param_name: contrail_config_node_ips
    - name: get contrail config node ips
      register: contrail_config_nodes_list
      shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
    - name: turn contrail config ips into string
      set_fact:
        contrail_config_nodes_string: '{{ contrail_config_nodes_list.stdout | regex_replace(''nil'',
          ''[]'') | from_json | join('','') }}'
    - name: set contrail config list to operator nodes if empty
      set_fact:
        contrail_config_nodes_string: '{{ contrail_operator_nodes_string }}'
      when: contrail_config_nodes_string == ''
    when: contrail_config_nodes_string == ''
  - name: config db node ips (names) param name
    set_fact:
      contrail_nodes_param_name: contrail_database_node_ips
  - name: get contrail config database node ips
    register: contrail_database_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
  - name: turn contrail config db ips into string
    set_fact:
      contrail_database_nodes_string: '{{ contrail_database_nodes_list.stdout | regex_replace(''nil'',
        ''[]'') | from_json | join('','') }}'
  - name: get external contrail control node ips
    register: contrail_control_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml contrail_control_ips
  - name: set contrail control ips
    set_fact:
      contrail_control_nodes_string: '{{ contrail_control_nodes_list.stdout | regex_replace(''nil'',
        '''') }}'
  - block:
    - name: config node ips (names) param name
      set_fact:
        contrail_nodes_param_name: contrail_control_node_ips
    - name: get contrail control node ips
      register: contrail_control_nodes_list
      shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
    - name: turn contrail control ips into string
      set_fact:
        contrail_control_nodes_string: '{{ contrail_control_nodes_list.stdout | regex_replace(''nil'',
          ''[]'') | from_json | join('','') }}'
    - name: set control list to operator nodes if empty
      set_fact:
        contrail_control_nodes_string: '{{ contrail_operator_nodes_string }}'
      when: contrail_control_nodes_string == ''
    when: contrail_control_nodes_string == ''
  - name: get external contrail analytics node ips
    register: contrail_analytics_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml contrail_analytics_ips
  - name: set contrail analytics ips
    set_fact:
      contrail_analytics_nodes_string: '{{ contrail_analytics_nodes_list.stdout |
        regex_replace(''nil'', '''') }}'
  - block:
    - name: analytics node ips (names) param name
      set_fact:
        contrail_nodes_param_name: contrail_analytics_node_ips
    - name: get contrail analytics node ips
      register: contrail_analytics_nodes_list
      shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
    - name: turn contrail analytics ips into string
      set_fact:
        contrail_analytics_nodes_string: '{{ contrail_analytics_nodes_list.stdout
          | regex_replace(''nil'', ''[]'') | from_json | join('','') }}'
    - name: set analytics list to operator nodes if empty
      set_fact:
        contrail_analytics_nodes_string: '{{ contrail_operator_nodes_string }}'
      when: contrail_analytics_nodes_string == ''
    when: contrail_analytics_nodes_string == ''
  - name: snmp node ips (names) param name
    set_fact:
      contrail_nodes_param_name: contrail_analytics_snmp_node_ips
  - name: get contrail analytics snmp node ips
    register: contrail_analytics_snmp_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
  - name: turn contrail analytics snmp ips into string
    set_fact:
      contrail_analytics_snmp_nodes_string: '{{ contrail_analytics_snmp_nodes_list.stdout
        | regex_replace(''nil'', ''[]'') | from_json | join('','') }}'
  - name: alarm node ips (names) param name
    set_fact:
      contrail_nodes_param_name: contrail_analytics_alarm_node_ips
  - name: get contrail analytics alarm node ips
    register: contrail_analytics_alarm_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
  - name: turn contrail analytics ips into string
    set_fact:
      contrail_analytics_alarm_nodes_string: '{{ contrail_analytics_alarm_nodes_list.stdout
        | regex_replace(''nil'', ''[]'') | from_json | join('','') }}'
  - name: analytics db node ips (names) param name
    set_fact:
      contrail_nodes_param_name: contrail_analytics_database_node_ips
  - name: get contrail analytics database node ips
    register: contrail_analytics_database_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
  - name: turn contrail analytics database ips into string
    set_fact:
      contrail_analytics_database_nodes_string: '{{ contrail_analytics_database_nodes_list.stdout
        | regex_replace(''nil'', ''[]'') | from_json | join('','') }}'
  - name: kafka services specific node ips param name
    set_fact:
      contrail_nodes_param_name: contrail_analytics_alarm_node_ips
  - name: get contrail service specific node ips
    register: contrail_service_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
  - name: turn contrail service specific node ips into string
    set_fact:
      contrail_kafka_service_nodes_string: '{{ contrail_service_nodes_list.stdout
        | regex_replace(''nil'', ''[]'') | from_json | join('','')  }}'
  - lineinfile:
      dest: /etc/contrail/common_contrail.env
      line: '{{ item.key }}={{ item.value }}'
      regexp: ^{{ item.key }}.*
      state: present
    name: write contrail config node ips to environment file
    with_dict:
      ANALYTICSDB_ENABLE: '{{ ''True'' if contrail_analytics_database_nodes_string
        != '''' else ''False'' }}'
      ANALYTICSDB_NODES: '{{ contrail_analytics_database_nodes_string }}'
      ANALYTICS_ALARM_ENABLE: '{{ ''True'' if contrail_analytics_alarm_nodes_string
        != '''' else ''False'' }}'
      ANALYTICS_ALARM_NODES: '{{ contrail_analytics_alarm_nodes_string }}'
      ANALYTICS_NODES: '{{ contrail_analytics_nodes_string }}'
      ANALYTICS_SNMP_ENABLE: '{{ ''True'' if contrail_analytics_snmp_nodes_string
        != '''' else ''False'' }}'
      ANALYTICS_SNMP_NODES: '{{ contrail_analytics_snmp_nodes_string }}'
      CONFIGDB_NODES: '{{ contrail_database_nodes_string }}'
      CONFIG_NODES: '{{ contrail_config_nodes_string }}'
      CONTROL_NODES: '{{ contrail_control_nodes_string }}'
      DNS_NODES: '{{ contrail_control_nodes_string }}'
      KAFKA_NODES: '{{ contrail_kafka_service_nodes_string }}'
      RABBITMQ_NODES: '{{ contrail_database_nodes_string }}'
      RABBITMQ_NODE_PORT: 5673
  - name: role specific contrail_settings
    set_fact:
      contrail_settings:
        BGP_ASN: 64520
        BGP_AUTO_MESH: true
        HEAP_NEWSIZE: 2G
        MAX_HEAP_SIZE: 16G
        VROUTER_ENCRYPTION: false
        VROUTER_GATEWAY: 10.1.110.1
  - name: get dmi
    register: dmi
    shell: '#!/bin/sh

      dmidecode --s system-uuid | awk ''match($0, /[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}/)
      { print substr($0, RSTART, RLENGTH) }''

      '
  - name: set dmi fact
    set_fact:
      dmi_uuid: '{{ dmi.stdout }}'
  - name: get node specific contrail_settings
    register: contrail_settings_string
    shell: hiera -c /etc/puppet/hiera.yaml contrail_settings ::uuid={{ dmi_uuid }}
      | sed 's/=>/:/g'
  - block:
    - set_fact:
        contrail_settings_json: '{{ contrail_settings_string.stdout | from_json }}'
    - set_fact:
        contrail_settings: '{{ contrail_settings | combine({item.key: item.value})
          }}'
      with_dict: '{{ contrail_settings_json }}'
    name: merge node and role specific contrail_settings
    when:
    - contrail_settings_string.stdout is defined
    - contrail_settings_string.stdout != "nil"
  - lineinfile:
      dest: /etc/contrail/common_contrail.env
      line: '{{ item.key }}={{ item.value }}'
      regexp: ^{{ item.key }}.*
      state: present
    name: write contrail settings to file
    when: contrail_settings is defined
    with_dict: '{{ contrail_settings }}'
  - block:
    - blockinfile:
        block: "module contrail_nodemgr 1.0;\n\nrequire {\n        type unconfined_t;\n\
          \        type container_share_t;\n        class file entrypoint;\n}\n\n\
          #============= unconfined_t ==============\n##!!!! The file '/usr/bin/conmon'\
          \ is mislabeled on your system.\n##!!!! Fix with $ restorecon -R -v /usr/bin/conmon\n\
          allow unconfined_t container_share_t:file entrypoint;\n"
        create: true
        path: /tmp/contrail_nodemgr.te
      name: create policy file for nodemgr
    - name: create contrail nodemgr selinux policy module
      shell: /bin/checkmodule -M -m -o /tmp/contrail_nodemgr.mod /tmp/contrail_nodemgr.te
    - name: create contrail nodemgr selinux policy package
      shell: /bin/semodule_package -o /tmp/contrail_nodemgr.pp -m /tmp/contrail_nodemgr.mod
    - name: install contrail nodemgr selinux policy package
      shell: /sbin/semodule -i /tmp/contrail_nodemgr.pp
    name: create selinux policy file for nodemgr
  - name: contrail_selfsigned_ca_flag
    set_fact:
      contrail_selfsigned_ca_flag: 'true'
  - block:
    - name: contrail cert file
      set_fact:
        contrail_cert_file: ''
    - name: contrail key file
      set_fact:
        contrail_certkey_file: ''
    - name: remove current cert files
      shell: rm -f {{ contrail_cert_file }} {{ contrail_certkey_file }}
    - name: re-run node-init container
      shell: "if {{ container_cli }} inspect contrail-node-init >/dev/null 2>&1 ;\
        \ then\n  {{ container_cli }} start -i contrail-node-init\nfi\n"
    name: Re-create Contrail self-signed certs
    when: '{{ contrail_selfsigned_ca_flag | bool }}'
  name: Contrail Base host prep tasks
- block:
  - blockinfile:
      block: '(block contrail_container

        (allow chkpwd_t container_file_t (lnk_file (read)))

        (allow container_t cloud_init_t ( unix_dgram_socket ( sendto )))

        (allow container_t container_file_t (chr_file (read)))

        (allow container_t container_share_t ( file ( entrypoint )))

        (allow container_t container_var_run_t (file ( openat open read write lock
        )))

        (allow container_t mount_var_run_t (dir ( openat read write )))

        (allow container_t spc_t (dir (getattr)))

        (allow container_t spc_t (fifo_file (write ioctl getattr setattr)))

        (allow container_t system_dbusd_t (dbus (send_msg)))

        (allow container_t var_t ( dir ( openat open read getattr lock search ioctl
        add_name remove_name write setattr create )))

        (allow container_t var_t ( file ( getattr read write append ioctl lock map
        open create setattr unlink )))

        (allow container_t var_t ( sock_file ( getattr read write append open )))

        (allow container_t var_lib_t ( dir ( openat open read getattr lock search
        ioctl add_name remove_name write setattr create )))

        (allow container_t var_lib_t ( file ( getattr read write append ioctl lock
        map open create setattr unlink )))

        (allow container_t var_lib_t ( sock_file ( getattr read write append open
        )))

        (allow chronyc_t container_file_t (lnk_file ( read )))

        (allow chronyd_t container_file_t (lnk_file ( read )))

        (allow chronyd_t container_file_t ( file ( open getattr read)))

        (allow chronyd_t spc_t ( unix_dgram_socket ( sendto )))

        (allow chronyd_t cloud_init_t ( unix_dgram_socket ( sendto )))

        (allow certmonger_t sudo_exec_t ( file ( execute execute_no_trans open read
        )))

        (allow certmonger_t container_file_t ( dir ( search open read getattr setattr
        add_name remove_name create )))

        (allow certmonger_t container_file_t ( file ( getattr read write append ioctl
        lock map open create setattr unlink )))

        (allow certmonger_t container_file_t ( lnk_file ( read )))

        (allow certmonger_t self ( capability ( sys_resource )))

        (allow certmonger_t self ( netlink_audit_socket ( create )))

        (allow certmonger_t sssd_conf_t ( dir ( search openat read write )))

        (allow certmonger_t sssd_conf_t ( file ( getattr read open )))

        (allow certmonger_t chkpwd_exec_t ( file ( execute )))

        (allow certmonger_t self  (process ( setrlimit )))

        (allow dhcpc_t container_file_t (file ( open getattr setattr read write)))

        (allow kmod_t container_file_t (system ( module_load )))

        (allow named_t container_file_t (file (open getattr read)))

        (allow NetworkManager_t container_file_t (file (read open)))

        (allow rhsmcertd_t container_file_t (file (open getattr read)))

        (allow setroubleshootd_t container_file_t (file (open getattr read)))

        (allow snmpd_t container_file_t (file (open read getattr)))

        (allow sshd_t container_file_t (lnk_file (read)))

        (allow sssd_t container_file_t (file (open getattr read)))

        (allow system_dbusd_t container_file_t (lnk_file (read)))

        (allow systemd_logind_t container_file_t (lnk_file (read)))

        (allow svirt_tcg_t container_file_t ( dir ( read  )))

        (allow timedatex_t container_file_t ( lnk_file ( read )))

        (allow systemd_sysctl_t container_file_t ( file ( getattr open read )))

        )

        '
      create: true
      marker: ; {mark} ANSIBLE MANAGED BLOCK
      path: /tmp/contrail_container.cil
    name: create selinux module file /tmp/contrail_container.cil
  - name: create selinux module from file /tmp/contrail_container.cil
    shell: /sbin/semodule -i /tmp/contrail_container.cil
  name: create selinux module for unprivileged contrail containers
- block:
  - name: register contrail dpdk env file
    register: p
    stat:
      path: /etc/contrail/common_vrouter.env
  - copy:
      content: ''
      dest: /etc/contrail/common_vrouter.env
    name: create contrail dpdk env file
    when: not p.stat.exists
  - name: contrail dpdk get metadata secret
    register: contrail_dpdk_metadata_secret
    shell: hiera -c /etc/puppet/hiera.yaml contrail::vrouter::metadata_proxy_shared_secret
  - name: contrail vrouter agent_mode
    set_fact:
      agent_mode: kernel
  - lineinfile:
      dest: /etc/contrail/common_vrouter.env
      line: '{{ item.key }}={{ item.value }}'
      regexp: ^{{ item.key }}.*
      state: present
    name: write remaining contrail dpdk env cars
    with_dict:
      AGENT_MODE: '{{ agent_mode }}'
      METADATA_PROXY_SECRET: '{{ contrail_dpdk_metadata_secret.stdout }}'
      NODE_TYPE: vrouter
  - copy:
      content: '#  This file is part of systemd.

        #

        #  systemd is free software; you can redistribute it and/or modify it

        #  under the terms of the GNU Lesser General Public License as published by

        #  the Free Software Foundation; either version 2.1 of the License, or

        #  (at your option) any later version.


        [Unit]

        Description=Huge Pages 1G pagesize File System

        Documentation=https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt

        Documentation=http://www.freedesktop.org/wiki/Software/systemd/APIFileSystems

        DefaultDependencies=no

        Before=sysinit.target

        ConditionPathExists=/sys/kernel/mm/hugepages/hugepages-1048576kB

        ConditionCapability=CAP_SYS_ADMIN

        ConditionKernelCommandLine=hugepagesz=1G


        [Mount]

        What=hugetlbfs

        Where=/dev/hugepages1G

        Type=hugetlbfs

        Options=pagesize=1G


        [Install]

        WantedBy=sysinit.target

        '
      dest: /usr/lib/systemd/system/dev-hugepages1G.mount
      mode: 420
    name: copy dev-hugepages1G.mount
  - copy:
      content: '#  This file is part of systemd.

        #

        #  systemd is free software; you can redistribute it and/or modify it

        #  under the terms of the GNU Lesser General Public License as published by

        #  the Free Software Foundation; either version 2.1 of the License, or

        #  (at your option) any later version.


        [Unit]

        Description=Huge Pages 2M pagesize File System

        Documentation=https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt

        Documentation=http://www.freedesktop.org/wiki/Software/systemd/APIFileSystems

        DefaultDependencies=no

        Before=sysinit.target

        ConditionPathExists=/sys/kernel/mm/hugepages/hugepages-2048kB

        ConditionCapability=CAP_SYS_ADMIN

        ConditionKernelCommandLine=hugepagesz=2M


        [Mount]

        What=hugetlbfs

        Where=/dev/hugepages2M

        Type=hugetlbfs

        Options=pagesize=2M


        [Install]

        WantedBy=sysinit.target

        '
      dest: /usr/lib/systemd/system/dev-hugepages2M.mount
      mode: 420
    name: copy dev-hugepages2M.mount
  - ignore_errors: true
    name: enable mount hugepages services
    shell: 'set -x

      systemctl enable {{ item }}

      systemctl start {{ item }}

      '
    with_items:
    - dev-hugepages1G.mount
    - dev-hugepages2M.mount
  - name: contrail vrouter hugepages_2mb
    set_fact:
      hugepages_2mb: ''
  - lineinfile:
      dest: /etc/contrail/common_vrouter.env
      line: HUGE_PAGES_2MB={{ hugepages_2mb }}
      regexp: HUGE_PAGES_2MB=
      state: present
    name: write hugepages_2mb if set
    when: hugepages_2mb != ""
  - lineinfile:
      dest: /etc/contrail/common_vrouter.env
      regexp: HUGE_PAGES_2MB=
      state: absent
    name: remove hugepages_2mb if unset
    when: hugepages_2mb == ""
  - name: contrail vrouter hugepages_1gb
    set_fact:
      hugepages_1gb: '2'
  - lineinfile:
      dest: /etc/contrail/common_vrouter.env
      line: HUGE_PAGES_1GB={{ hugepages_1gb }}
      regexp: HUGE_PAGES_1GB=
      state: present
    name: write hugepages_1gb if set
    when: hugepages_1gb != ""
  - lineinfile:
      dest: /etc/contrail/common_vrouter.env
      regexp: HUGE_PAGES_1GB=
      state: absent
    name: remove hugepages_1gb if unset
    when: hugepages_1gb == ""
  - name: default hugepages dir
    set_fact:
      hp_dir: /dev/hugepages
  - name: 1gb hugepages dir
    register: hp_1g_dir
    shell: mount -t hugetlbfs | awk '/pagesize=1G/{print($3)}'
  - name: 2mb hugepages dir
    register: hp_2mb_dir
    shell: mount -t hugetlbfs | awk '/pagesize=2M/{print($3)}'
  - name: set hugepages dir to 1gb
    set_fact:
      hp_dir: '{{ hp_1g_dir.stdout }}'
    when:
    - hp_1g_dir.stdout != ""
  - name: set hugepages dir to 2mb
    set_fact:
      hp_dir: '{{ hp_2mb_dir.stdout }}'
    when:
    - hugepages_1gb == ""
    - hp_1g_dir.stdout == ""
    - hp_2mb_dir.stdout != ""
  - lineinfile:
      dest: /etc/contrail/common_vrouter.env
      line: HUGE_PAGES_DIR={{ hp_dir }}
      regexp: HUGE_PAGES_DIR=
      state: present
    name: set hugepages dir
  name: Contrail vrouter base host prep tasks
- name: service config name
  set_fact:
    contrail_conf_name: vrouter
- block:
  - name: common service env file name
    set_fact:
      contrail_service_common_env_file: /etc/contrail/common_contrail.env
  - name: service env file path
    set_fact:
      contrail_service_env_file: /etc/contrail/common_{{ contrail_conf_name }}.env
  - name: service config dir
    set_fact:
      contrail_service_conf_dir: /var/lib/config-data/contrail_vrouter
  - name: service config md5 sum file
    set_fact:
      contrail_service_conf_md5_file: '{{ contrail_service_conf_dir }}.md5sum'
  - name: save md5 service env files and common env
    register: config_md5sum
    shell: '#!/bin/bash -x

      mkdir -p {{ contrail_service_conf_dir }}

      rm -f {{ contrail_service_conf_dir }}/*

      cp {{ contrail_service_common_env_file }} {{ contrail_service_env_file }} {{
      contrail_service_conf_dir }}/

      vars=$(cat {{ contrail_service_common_env_file }} {{ contrail_service_env_file
      }} | sort -u)

      echo "$vars" | md5sum | awk ''{print($1)}'' > {{ contrail_service_conf_md5_file
      }}

      cat {{ contrail_service_conf_md5_file }}

      chmod 660 {{ contrail_service_conf_dir }}/*

      '
  - debug:
      msg: 'service config md5sum: service={{ contrail_conf_name }} md5={{ config_md5sum.stdout
        }}'
  name: Contrail config volume prep tasks
  when: contrail_conf_name != ''
- file:
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - path: /etc/iscsi
    setype: svirt_sandbox_file_t
  - path: /etc/target
    setype: svirt_sandbox_file_t
  - path: /var/lib/iscsi
    setype: svirt_sandbox_file_t
- name: stat /lib/systemd/system/iscsid.socket
  register: stat_iscsid_socket
  stat: path=/lib/systemd/system/iscsid.socket
- name: Stop and disable iscsid.socket service
  service: name=iscsid.socket state=stopped enabled=no
  when: stat_iscsid_socket.stat.exists
- command: systemctl is-enabled --quiet iscsi.service
  failed_when: false
  name: Check if iscsi.service is enabled
  register: iscsi_service_enabled_result
- name: Stop iscsi.service
  service: name=iscsi.service state=stopped enabled=no
  when:
  - iscsi_service_enabled_result is changed
  - iscsi_service_enabled_result.rc == 0
- include_role:
    name: tripleo-kernel
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent logs directory
  with_items:
  - mode: '0750'
    path: /var/log/containers/metrics_qdr
    setype: svirt_sandbox_file_t
  - path: /var/lib/metrics_qdr
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/nova
    setype: svirt_sandbox_file_t
- file:
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - path: /var/lib/nova
    setype: svirt_sandbox_file_t
  - path: /var/lib/_nova_secontext
    setype: svirt_sandbox_file_t
  - path: /var/lib/nova/instances
    setype: svirt_sandbox_file_t
  - path: /var/lib/libvirt
    setype: svirt_sandbox_file_t
- mount: name=/var/lib/nova/instances src="{{nfs_share}}" fstype=nfs4 opts="_netdev,bg,{{nfs_options}},vers={{nfs_vers}},nfsvers={{nfs_vers}}"
    state=mounted
  name: Mount Nova NFS Share
  vars:
    nfs_backend_enable: false
    nfs_options: context=system_u:object_r:nfs_t:s0
    nfs_share: ''
    nfs_vers: '4'
  when: nfs_backend_enable|bool
- name: is Nova Resume Guests State On Host Boot enabled
  set_fact:
    resume_guests_state_on_host_boot_enabled: false
- block:
  - copy:
      content: '[Unit]

        Description=Suspend/Resume Running libvirt Guests

        After=network.target

        After=time-sync.target

        After=virt-guest-shutdown.target

        After=docker.service

        After=paunch-container-shutdown.service

        After=rhel-push-plugin.service

        Documentation=man:libvirtd(8)

        Documentation=https://libvirt.org


        [Service]

        EnvironmentFile=-/var/lib/config-data/puppet-generated/nova_libvirt/etc/sysconfig/libvirt-guests

        # Hack just call traditional service until we factor

        # out the code

        ExecStart=/bin/{{container_cli}} exec nova_libvirt /bin/sh -x /usr/libexec/libvirt-guests.sh
        start

        ExecStop=/bin/{{container_cli}} stop nova_compute

        ExecStop=/bin/{{container_cli}}  exec nova_libvirt /bin/sh -x /usr/libexec/libvirt-guests.sh
        stop

        Type=oneshot

        RemainAfterExit=yes

        StandardOutput=journal+console

        TimeoutStopSec=0


        [Install]

        WantedBy=multi-user.target

        '
      dest: /etc/systemd/system/libvirt-guests.service
    name: libvirt-guests unit to stop nova_compute container before shutdown VMs
  - copy:
      content: '[Unit]

        Description=Libvirt guests shutdown

        Documentation=https://libvirt.org

        '
      dest: /etc/systemd/system/virt-guest-shutdown.target
    name: Making sure virt-guest-shutdown.target is present
  - name: libvirt-guests enable VM shutdown on compute reboot/shutdown
    systemd:
      daemon_reload: true
      enabled: true
      name: libvirt-guests
  name: install libvirt-guests systemd unit file (docker)
  when:
  - resume_guests_state_on_host_boot_enabled|bool
  - container_cli == 'docker'
- block:
  - copy:
      content: '[Unit]

        Description=Suspend libvirt Guests in tripleo

        Requires=virt-guest-shutdown.target

        After=systemd-machined.service

        After=tripleo_nova_libvirt.service

        Before=tripleo_nova_compute.service

        Documentation=man:libvirtd(8)

        Documentation=https://libvirt.org


        [Service]

        EnvironmentFile=-/etc/sysconfig/libvirt-guests

        ExecStart=/bin/{{container_cli}} exec nova_libvirt /bin/rm -f /var/lib/libvirt/libvirt-guests

        ExecStop=/bin/{{container_cli}} exec nova_libvirt /bin/sh -x /usr/libexec/libvirt-guests.sh
        shutdown

        Type=oneshot

        RemainAfterExit=yes

        StandardOutput=journal+console

        TimeoutStopSec=0


        [Install]

        WantedBy=multi-user.target

        '
      dest: /etc/systemd/system/tripleo_nova_libvirt_guests.service
    name: libvirt-guests unit to stop nova_compute container before shutdown VMs
  - copy:
      content: '[Unit]

        Description=Libvirt guests shutdown

        Documentation=https://libvirt.org

        '
      dest: /etc/systemd/system/virt-guest-shutdown.target
    name: Making sure virt-guest-shutdown.target is present
  - name: tripleo_nova_libvirt_guests enable VM shutdown on compute reboot/shutdown
    systemd:
      daemon_reload: true
      enabled: true
      name: tripleo_nova_libvirt_guests
  name: install tripleo_nova_libvirt_guests systemd unit file (podman)
  when:
  - resume_guests_state_on_host_boot_enabled|bool
  - container_cli == 'podman'
- file:
    path: /etc/ceph
    state: directory
  name: ensure ceph configurations exist
- name: is Instance HA enabled
  set_fact:
    instance_ha_enabled: false
- block:
  - file:
      path: /var/lib/nova/instanceha
      state: directory
    name: prepare Instance HA script directory
  - copy:
      content: "#!/usr/libexec/platform-python\n\nimport os\nimport sys\nimport time\n\
        import inspect\nimport logging\nimport argparse\nimport oslo_config.cfg\n\
        import requests.exceptions\n\ndef is_forced_down(connection, hostname):\n\
        \    services = connection.services.list(host=hostname, binary=\"nova-compute\"\
        )\n    for service in services:\n        if service.forced_down:\n       \
        \     return True\n    return False\n\ndef evacuations_done(connection, hostname):\n\
        \    # Get a list of migrations.\n    #  :param host: (optional) filter migrations\
        \ by host name.\n    #  :param status: (optional) filter migrations by status.\n\
        \    #  :param cell_name: (optional) filter migrations for a cell.\n    #\n\
        \    migrations = connection.migrations.list(host=hostname)\n\n    print(\"\
        Checking %d migrations\" % len(migrations))\n    for migration in migrations:\n\
        \        # print migration.to_dict()\n        #\n        # {\n        # u'status':\
        \ u'error',\n        # u'dest_host': None,\n        # u'new_instance_type_id':\
        \ 2,\n        # u'old_instance_type_id': 2,\n        # u'updated_at': u'2018-04-22T20:55:29.000000',\n\
        \        # u'dest_compute':\n        #   u'overcloud-novacompute-2.localdomain',\n\
        \        # u'migration_type': u'live-migration',\n        # u'source_node':\n\
        \        #   u'overcloud-novacompute-0.localdomain',\n        # u'id': 8,\n\
        \        # u'created_at': u'2018-04-22T20:52:58.000000',\n        # u'instance_uuid':\n\
        \        #   u'd1c82ce8-3dc5-48db-b59f-854b3b984ef1',\n        # u'dest_node':\n\
        \        #   u'overcloud-novacompute-2.localdomain',\n        # u'source_compute':\n\
        \        #   u'overcloud-novacompute-0.localdomain'\n        # }\n       \
        \ # Acceptable: done, completed, failed\n        if migration.status in [\"\
        running\", \"accepted\", \"pre-migrating\"]:\n            return False\n \
        \   return True\n\ndef safe_to_start(connection, hostname):\n    if is_forced_down(connection,\
        \ hostname):\n        print(\"Waiting for fence-down flag to be cleared\"\
        )\n        return False\n    if not evacuations_done(connection, hostname):\n\
        \        print(\"Waiting for evacuations to complete or fail\")\n        return\
        \ False\n    return True\n\ndef create_nova_connection(options):\n    try:\n\
        \        from novaclient import client\n        from novaclient.exceptions\
        \ import NotAcceptable\n    except ImportError:\n        print(\"Nova not\
        \ found or not accessible\")\n        sys.exit(1)\n\n    from keystoneauth1\
        \ import loading\n    from keystoneauth1 import session\n    from keystoneclient\
        \ import discover\n\n    # Prefer the oldest and strip the leading 'v'\n \
        \   keystone_versions = discover.available_versions(options[\"auth_url\"][0])\n\
        \    keystone_version = keystone_versions[0]['id'][1:]\n    kwargs = dict(\n\
        \        auth_url=options[\"auth_url\"][0],\n        username=options[\"username\"\
        ][0],\n        password=options[\"password\"][0]\n        )\n\n    if discover.version_match(\"\
        2\", keystone_version):\n        kwargs[\"tenant_name\"] = options[\"tenant_name\"\
        ][0]\n\n    elif discover.version_match(\"3\", keystone_version):\n      \
        \  kwargs[\"project_name\"] = options[\"project_name\"][0]\n        kwargs[\"\
        user_domain_name\"] = options[\"user_domain_name\"][0]\n        kwargs[\"\
        project_domain_name\"] = options[\"project_domain_name\"][0]\n\n    loader\
        \ = loading.get_plugin_loader('password')\n    keystone_auth = loader.load_from_options(**kwargs)\n\
        \    keystone_session = session.Session(auth=keystone_auth, verify=(not options[\"\
        insecure\"]))\n\n    nova_endpoint_type = 'internalURL'\n    # We default\
        \ to internalURL but we allow this to be overridden via\n    # the [placement]/os_interface\
        \ key.\n    if 'os_interface' in options and len(options[\"os_interface\"\
        ]) == 1:\n        nova_endpoint_type = options[\"os_interface\"][0]\n    #\
        \ Via https://review.opendev.org/#/c/492247/ os_interface has been deprecatd\
        \ in queens\n    # and we need to use 'valid_interfaces' which is a:\n   \
        \ # \"List of interfaces, in order of preference, for endpoint URL. (list\
        \ value)\"\n    # Since it is not explicitely set in nova.conf we still keep\
        \ the check for os_interface\n    elif 'valid_interfaces' in options and len(options[\"\
        valid_interfaces\"]) >= 1:\n        nova_endpoint_type = options[\"valid_interfaces\"\
        ][0]\n\n    # This mimicks the code in novaclient/shell.py\n    if nova_endpoint_type\
        \ in ['internal', 'public', 'admin']:\n        nova_endpoint_type += 'URL'\n\
        \n    if 'region_name' in options:\n        region = options['region_name'][0]\n\
        \    elif 'os_region_name' in options:\n        region = options['os_region_name'][0]\n\
        \    else: # We actually try to make a client call even with an empty region\n\
        \        region = None\n    nova_versions = [ \"2.23\", \"2\" ]\n    for version\
        \ in nova_versions:\n        clientargs = inspect.getargspec(client.Client).varargs\n\
        \        # Some versions of Openstack prior to Ocata only\n        # supported\
        \ positional arguments for username,\n        # password, and tenant.\n  \
        \      #\n        # Versions since Ocata only support named arguments.\n \
        \       #\n        # So we need to use introspection to figure out how to\n\
        \        # create a Nova client.\n        #\n        # Happy days\n      \
        \  #\n        if clientargs:\n            # OSP < Ocata\n            # ArgSpec(args=['version',\
        \ 'username', 'password', 'project_id', 'auth_url'],\n            #      \
        \   varargs=None,\n            #         keywords='kwargs', defaults=(None,\
        \ None, None, None))\n            nova = client.Client(version,\n        \
        \                         None, # User\n                                 None,\
        \ # Password\n                                 None, # Tenant\n          \
        \                       None, # Auth URL\n                               \
        \  insecure=options[\"insecure\"],\n                                 region_name=region,\n\
        \                                 session=keystone_session, auth=keystone_auth,\n\
        \                                 http_log_debug=\"verbose\" in options,\n\
        \                                 endpoint_type=nova_endpoint_type)\n    \
        \    else:\n            # OSP >= Ocata\n            # ArgSpec(args=['version'],\
        \ varargs='args', keywords='kwargs', defaults=None)\n            nova = client.Client(version,\n\
        \                                 region_name=region,\n                  \
        \               session=keystone_session, auth=keystone_auth,\n          \
        \                       http_log_debug=\"verbose\" in options,\n         \
        \                        endpoint_type=nova_endpoint_type)\n\n        try:\n\
        \            nova.hypervisors.list()\n            return nova\n\n        except\
        \ NotAcceptable as e:\n            logging.warning(e)\n\n        except Exception\
        \ as e:\n            logging.warning(\"Nova connection failed. %s: %s\" %\
        \ (e.__class__.__name__, e))\n\n    print(\"Couldn't obtain a supported connection\
        \ to nova, tried: %s\\n\" % repr(nova_versions))\n    return None\n\n\nparser\
        \ = argparse.ArgumentParser(description='Process some integers.')\nparser.add_argument('--config-file',\
        \ dest='nova_config', action='store',\n                    default=\"/etc/nova/nova.conf\"\
        ,\n                    help='path to nova configuration (default: /etc/nova/nova.conf)')\n\
        parser.add_argument('--nova-binary', dest='nova_binary', action='store',\n\
        \                    default=\"/usr/bin/nova-compute\",\n                \
        \    help='path to nova compute binary (default: /usr/bin/nova-compute)')\n\
        parser.add_argument('--enable-file', dest='enable_file', action='store',\n\
        \                    default=\"/var/lib/nova/instanceha/enabled\",\n     \
        \               help='file exists if instance HA is enabled on this host '\\\
        \n                    '(default: /var/lib/nova/instanceha/enabled)')\n\n\n\
        sections = {}\n(args, remaining) = parser.parse_known_args(sys.argv)\n\nconfig\
        \ = oslo_config.cfg.ConfigParser(args.nova_config, sections)\nconfig.parse()\n\
        config.sections[\"placement\"][\"insecure\"] = 0\nconfig.sections[\"placement\"\
        ][\"verbose\"] = 1\n\nif os.path.isfile(args.enable_file):\n    connection\
        \ = None\n    while not connection:\n        # Loop in case the control plane\
        \ is recovering when we run\n        connection = create_nova_connection(config.sections[\"\
        placement\"])\n        if not connection:\n            time.sleep(10)\n\n\
        \    while not safe_to_start(connection, config.sections[\"DEFAULT\"][\"host\"\
        ][0]):\n        time.sleep(10)\n\nreal_args = [args.nova_binary, '--config-file',\
        \ args.nova_config]\nreal_args.extend(remaining[1:])\nos.execv(args.nova_binary,\
        \ real_args)\n"
      dest: /var/lib/nova/instanceha/check-run-nova-compute
      mode: 493
    name: install Instance HA script that runs nova-compute
  - command: hiera -c /etc/puppet/hiera.yaml compute_instanceha_short_node_names
    name: Get list of instance HA compute nodes
    register: iha_nodes
  - file: path=/var/lib/nova/instanceha/enabled state=touch
    name: If instance HA is enabled on the node activate the evacuation completed
      check
    when: iha_nodes.stdout|lower is search('"'+ansible_facts['hostname']|lower+'"')
  name: install Instance HA recovery script
  when: instance_ha_enabled|bool
- name: Is irqbalance enabled
  set_fact:
    compute_irqbalance_disabled: false
- name: disable irqbalance service on compute
  service:
    enabled: false
    name: irqbalance.service
    state: stopped
  when: compute_irqbalance_disabled|bool
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/libvirt
    setype: svirt_sandbox_file_t
- file:
    path: '{{ item.path }}'
    setype: '{{ item.setype | default(omit) }}'
    state: directory
  name: create libvirt persistent data directories
  with_items:
  - path: /etc/libvirt
    setype: svirt_sandbox_file_t
  - path: /etc/libvirt/secrets
    setype: svirt_sandbox_file_t
  - path: /etc/libvirt/qemu
    setype: svirt_sandbox_file_t
  - path: /var/lib/libvirt
    setype: svirt_sandbox_file_t
  - path: /var/cache/libvirt
  - path: /var/lib/nova
    setype: svirt_sandbox_file_t
  - path: /var/run/libvirt
    setype: virt_var_run_t
  - path: /var/log/libvirt
    setype: svirt_sandbox_file_t
  - path: /var/log/libvirt/qemu
    setype: svirt_sandbox_file_t
- group:
    gid: 107
    name: qemu
    state: present
  name: ensure qemu group is present on the host
- name: ensure qemu user is present on the host
  user:
    comment: qemu user
    group: qemu
    name: qemu
    shell: /sbin/nologin
    state: present
    uid: 107
- file:
    group: qemu
    owner: qemu
    path: /var/lib/vhost_sockets
    setype: virt_cache_t
    seuser: system_u
    state: directory
  name: create directory for vhost-user sockets with qemu ownership
- check_mode: false
  command: /usr/bin/rpm -q libvirt-daemon
  failed_when: false
  name: check if libvirt is installed
  register: libvirt_installed
- name: make sure libvirt services are disabled and masked
  service:
    daemon_reload: true
    enabled: false
    masked: true
    name: '{{ item }}'
    state: stopped
  when: libvirt_installed.rc == 0
  with_items:
  - libvirtd.service
  - virtlogd.socket
- copy:
    content: 'd /var/run/libvirt 0755 root root - -

      '
    dest: /etc/tmpfiles.d/var-run-libvirt.conf
  name: ensure /var/run/libvirt is present upon reboot
- file:
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  loop:
  - path: /var/run/libvirt
    setype: virt_var_run_t
  name: Create libvirt persistent data directories
- name: allow logrotate to read inside containers
  seboolean:
    name: logrotate_read_inside_containers
    persistent: true
    state: true
- block:
  - name: Set login facts
    set_fact:
      container_default_pids_limit: 4096
      container_events_logger_mechanism: journald
      container_registry_insecure_registries:
      - satellite.mgmt.tmeprj2.comm.red.ibm.gsc:5000
      container_registry_login: true
      container_registry_logins: {}
      container_registry_logins_json: {}
  - name: Convert logins json to dict
    set_fact:
      container_registry_logins: '{{ container_registry_logins_json | from_json }}'
    when:
    - container_registry_logins_json is string
    - container_registry_login | bool
    - (container_registry_logins_json | length) > 0
  - name: Set registry logins
    set_fact:
      container_registry_logins: '{{ container_registry_logins_json }}'
    when:
    - container_registry_logins_json is mapping
    - container_registry_login | bool
    - (container_registry_logins_json | length) > 0
  - include_role:
      name: tripleo-podman
      tasks_from: tripleo_podman_install.yml
    name: Run podman install
    vars:
      tripleo_container_default_pids_limit: '{{ container_default_pids_limit }}'
      tripleo_container_events_logger_mechanism: '{{ container_events_logger_mechanism
        }}'
      tripleo_container_registry_insecure_registries: '{{ container_registry_insecure_registries
        }}'
  - include_role:
      name: tripleo-podman
      tasks_from: tripleo_podman_login.yml
    name: Run podman login
    vars:
      tripleo_container_registry_login: '{{ container_registry_login | bool }}'
      tripleo_container_registry_logins: '{{ container_registry_logins }}'
  name: Install and configure Podman
- copy:
    content: 'This file makes paunch generate additional systemd

      dependencies for containers that have special

      start/stop ordering constraints. It ensures that

      those constraints are enforced on reboot/shutdown.

      '
    dest: /etc/sysconfig/podman_drop_in
  name: Configure paunch to generate systemd drop-in dependencies
- import_role:
    name: redhat-subscription
  name: Red Hat Subscription Management configuration during deployment
  vars:
    rhsm_activation_key: osp16-2-dev-overcloud
    rhsm_baseurl: https://satellite.mgmt.tmeprj2.comm.red.ibm.gsc/pulp/repos
    rhsm_force_register: true
    rhsm_method: satellite
    rhsm_org_id: default
    rhsm_release: 8.4
    rhsm_server_hostname: satellite.mgmt.tmeprj2.comm.red.ibm.gsc
- file:
    mode: '0750'
    path: /var/log/containers/rsyslog
    setype: svirt_sandbox_file_t
    state: directory
  name: create persistent logs directory for rsyslog
- file:
    path: /var/lib/rsyslog.container
    setype: svirt_sandbox_file_t
    state: directory
  name: create persistent state directory for rsyslog
- become: true
  failed_when: false
  name: Check for NTP service
  register: ntp_service_check
  shell: systemctl is-active ntpd.service || systemctl is-enabled ntpd.service
- name: Disable NTP before configuring Chrony
  service:
    enabled: false
    name: ntpd
    state: stopped
  when:
  - ntp_service_check.rc is defined
  - ntp_service_check.rc == 0
- include_role:
    name: chrony
  name: Install, Configure and Run Chrony
- meta: flush_handlers
  name: Ensure chrony has been restarted
- command: chronyc makestep
  name: Ensure system is NTP time synced
- name: Set timezone fact
  set_fact:
    timezone: UTC
- name: Set timezone to {{ timezone | default('UTC') }}
  register: timezone_result
  timezone:
    name: '{{ timezone }}'
- failed_when: false
  name: Restart services
  service:
    name: '{{ item }}'
    state: restarted
  when:
  - timezone_result.changed
  with_items:
  - rsyslog
  - crond
- include_role:
    name: tuned
