- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/ceilometer
    setype: svirt_sandbox_file_t
- name: enable virt_sandbox_use_netlink for healthcheck
  seboolean:
    name: virt_sandbox_use_netlink
    persistent: true
    state: true
- copy:
    content: '#!/bin/bash

      /var/lib/container-config-scripts/pacemaker_mutex_restart_bundle.sh --lock $*
      2>&1 | logger -t certmonger'
    dest: /usr/bin/certmonger-ha-resource-refresh.sh
    mode: '0700'
    setype: certmonger_unconfined_exec_t
  name: create certificate rotation script for HA services
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/cinder
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/httpd/cinder-api
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/cinder
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/cinder
    setype: svirt_sandbox_file_t
  - path: /var/lib/cinder
    setype: svirt_sandbox_file_t
- file:
    path: /etc/ceph
    state: directory
  name: ensure ceph configurations exist
- name: cinder_enable_iscsi_backend fact
  set_fact:
    cinder_enable_iscsi_backend: true
- block:
  - name: ensure LVM rpm dependencies are installed
    package:
      name: lvm2
      state: latest
  - args:
      creates: /var/lib/cinder/cinder-volumes
    command: dd if=/dev/zero of=/var/lib/cinder/cinder-volumes bs=1 count=0 seek=10280M
    name: cinder create LVM volume group dd
  - args:
      executable: /bin/bash
    changed_when: _loopback_device.rc == 2
    failed_when: _loopback_device.rc not in [0,2]
    name: Get or create LVM loopback device
    register: _loopback_device
    shell: "exit_code=0\nexisting_device=$(losetup -j /var/lib/cinder/cinder-volumes\
      \ -l -O NAME | tail -n-1)\nif [[ -z \"${existing_device}\" ]]; then\n    losetup\
      \ -f /var/lib/cinder/cinder-volumes --show\n    exit_code=2\nelse\n    echo\
      \ ${existing_device%$'\\n'*}\nfi\nexit ${exit_code}"
  - lvg:
      pvs: '{{ _loopback_device.stdout }}'
      state: present
      vg: cinder-volumes
    name: Create LVM volume group
    when:
    - not (ansible_check_mode | bool)
  - copy:
      content: '[Unit]

        Description=Cinder LVM losetup

        DefaultDependencies=no

        Conflicts=umount.target

        Requires=lvm2-monitor.service systemd-udev-settle.service

        Before=local-fs.target umount.target

        After=var.mount lvm2-monitor.service systemd-udev-settle.service


        [Service]

        Type=oneshot

        ExecStart=/sbin/losetup {{ _loopback_device.stdout }} /var/lib/cinder/cinder-volumes

        ExecStop=/sbin/losetup -d {{ _loopback_device.stdout }}

        RemainAfterExit=yes


        [Install]

        WantedBy=local-fs-pre.target

        '
      dest: /etc/systemd/system/cinder-lvm-losetup.service
    name: cinder create service to run losetup for LVM on startup
    when:
    - not (ansible_check_mode | bool)
  - name: cinder enable the LVM losetup service
    systemd:
      daemon_reload: true
      enabled: true
      name: cinder-lvm-losetup
  when: cinder_enable_iscsi_backend|bool
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/collectd
    setype: svirt_sandbox_file_t
- include_role:
    name: tripleo_provision_mcelog
  name: import provision_mcelog
  when: false
- block:
  - name: register common env file
    register: p
    stat:
      path: /etc/contrail/common_contrail.env
  - copy:
      content: ''
      dest: /etc/contrail/common_contrail.env
    name: create common env file
    when: not p.stat.exists
  - name: set contrail base calculated parameters
    set_fact:
      contrail_base_env_params:
        AAA_MODE: cloud-admin
        CLOUD_ORCHESTRATOR: openstack
        OPENSTACK_VERSION: train
        REDIS_SERVER_PORT: 6379
  - lineinfile:
      dest: /etc/contrail/common_contrail.env
      line: '{{ item.key }}={{ item.value }}'
      regexp: ^{{ item.key }}.*
      state: present
    name: write contrail base parameters to environment file
    with_dict: '{{ contrail_base_env_params }}'
  - name: get external contrail config node ips
    register: contrail_config_ips
    shell: hiera -c /etc/puppet/hiera.yaml contrail_config_ips
  - name: turn contrail config ips into string
    set_fact:
      contrail_config_ips_arr: '{{ contrail_config_ips.stdout | regex_replace(''nil'',
        ''[]'') | to_json | from_json }}'
  - set_fact:
      contrail_config_ips_string: '{{ contrail_config_ips_string | default('''')  +
        '' '' + item }}'
    with_items: '{{ contrail_config_ips_arr }}'
  - set_fact:
      contrail_config_ips_string: '{{ contrail_config_ips_string | default('''') |
        trim | regex_replace('' '', '','') }}'
  - name: contrail operator node ips (names) param name
    set_fact:
      contrail_nodes_param_name: contrail_operator_node_ips
  - name: get contrail operator node ips
    register: contrail_operator_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
  - name: turn contrail operator ips into string
    set_fact:
      contrail_operator_nodes_string: '{{ contrail_operator_nodes_list.stdout | regex_replace(''nil'',
        ''[]'') | from_json | join('','') }}'
  - name: set contrail config ips
    set_fact:
      contrail_config_nodes_string: '{{ contrail_config_ips_string }}'
  - block:
    - name: config api node ips (names) param name
      set_fact:
        contrail_nodes_param_name: contrail_config_node_ips
    - name: get contrail config node ips
      register: contrail_config_nodes_list
      shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
    - name: turn contrail config ips into string
      set_fact:
        contrail_config_nodes_string: '{{ contrail_config_nodes_list.stdout | regex_replace(''nil'',
          ''[]'') | from_json | join('','') }}'
    - name: set contrail config list to operator nodes if empty
      set_fact:
        contrail_config_nodes_string: '{{ contrail_operator_nodes_string }}'
      when: contrail_config_nodes_string == ''
    when: contrail_config_nodes_string == ''
  - name: config db node ips (names) param name
    set_fact:
      contrail_nodes_param_name: contrail_database_node_ips
  - name: get contrail config database node ips
    register: contrail_database_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
  - name: turn contrail config db ips into string
    set_fact:
      contrail_database_nodes_string: '{{ contrail_database_nodes_list.stdout | regex_replace(''nil'',
        ''[]'') | from_json | join('','') }}'
  - name: get external contrail control node ips
    register: contrail_control_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml contrail_control_ips
  - name: set contrail control ips
    set_fact:
      contrail_control_nodes_string: '{{ contrail_control_nodes_list.stdout | regex_replace(''nil'',
        '''') }}'
  - block:
    - name: config node ips (names) param name
      set_fact:
        contrail_nodes_param_name: contrail_control_node_ips
    - name: get contrail control node ips
      register: contrail_control_nodes_list
      shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
    - name: turn contrail control ips into string
      set_fact:
        contrail_control_nodes_string: '{{ contrail_control_nodes_list.stdout | regex_replace(''nil'',
          ''[]'') | from_json | join('','') }}'
    - name: set control list to operator nodes if empty
      set_fact:
        contrail_control_nodes_string: '{{ contrail_operator_nodes_string }}'
      when: contrail_control_nodes_string == ''
    when: contrail_control_nodes_string == ''
  - name: get external contrail analytics node ips
    register: contrail_analytics_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml contrail_analytics_ips
  - name: set contrail analytics ips
    set_fact:
      contrail_analytics_nodes_string: '{{ contrail_analytics_nodes_list.stdout |
        regex_replace(''nil'', '''') }}'
  - block:
    - name: analytics node ips (names) param name
      set_fact:
        contrail_nodes_param_name: contrail_analytics_node_ips
    - name: get contrail analytics node ips
      register: contrail_analytics_nodes_list
      shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
    - name: turn contrail analytics ips into string
      set_fact:
        contrail_analytics_nodes_string: '{{ contrail_analytics_nodes_list.stdout
          | regex_replace(''nil'', ''[]'') | from_json | join('','') }}'
    - name: set analytics list to operator nodes if empty
      set_fact:
        contrail_analytics_nodes_string: '{{ contrail_operator_nodes_string }}'
      when: contrail_analytics_nodes_string == ''
    when: contrail_analytics_nodes_string == ''
  - name: snmp node ips (names) param name
    set_fact:
      contrail_nodes_param_name: contrail_analytics_snmp_node_ips
  - name: get contrail analytics snmp node ips
    register: contrail_analytics_snmp_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
  - name: turn contrail analytics snmp ips into string
    set_fact:
      contrail_analytics_snmp_nodes_string: '{{ contrail_analytics_snmp_nodes_list.stdout
        | regex_replace(''nil'', ''[]'') | from_json | join('','') }}'
  - name: alarm node ips (names) param name
    set_fact:
      contrail_nodes_param_name: contrail_analytics_alarm_node_ips
  - name: get contrail analytics alarm node ips
    register: contrail_analytics_alarm_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
  - name: turn contrail analytics ips into string
    set_fact:
      contrail_analytics_alarm_nodes_string: '{{ contrail_analytics_alarm_nodes_list.stdout
        | regex_replace(''nil'', ''[]'') | from_json | join('','') }}'
  - name: analytics db node ips (names) param name
    set_fact:
      contrail_nodes_param_name: contrail_analytics_database_node_ips
  - name: get contrail analytics database node ips
    register: contrail_analytics_database_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
  - name: turn contrail analytics database ips into string
    set_fact:
      contrail_analytics_database_nodes_string: '{{ contrail_analytics_database_nodes_list.stdout
        | regex_replace(''nil'', ''[]'') | from_json | join('','') }}'
  - name: kafka services specific node ips param name
    set_fact:
      contrail_nodes_param_name: contrail_analytics_alarm_node_ips
  - name: get contrail service specific node ips
    register: contrail_service_nodes_list
    shell: hiera -c /etc/puppet/hiera.yaml {{ contrail_nodes_param_name }}
  - name: turn contrail service specific node ips into string
    set_fact:
      contrail_kafka_service_nodes_string: '{{ contrail_service_nodes_list.stdout
        | regex_replace(''nil'', ''[]'') | from_json | join('','')  }}'
  - lineinfile:
      dest: /etc/contrail/common_contrail.env
      line: '{{ item.key }}={{ item.value }}'
      regexp: ^{{ item.key }}.*
      state: present
    name: write contrail config node ips to environment file
    with_dict:
      ANALYTICSDB_ENABLE: '{{ ''True'' if contrail_analytics_database_nodes_string
        != '''' else ''False'' }}'
      ANALYTICSDB_NODES: '{{ contrail_analytics_database_nodes_string }}'
      ANALYTICS_ALARM_ENABLE: '{{ ''True'' if contrail_analytics_alarm_nodes_string
        != '''' else ''False'' }}'
      ANALYTICS_ALARM_NODES: '{{ contrail_analytics_alarm_nodes_string }}'
      ANALYTICS_NODES: '{{ contrail_analytics_nodes_string }}'
      ANALYTICS_SNMP_ENABLE: '{{ ''True'' if contrail_analytics_snmp_nodes_string
        != '''' else ''False'' }}'
      ANALYTICS_SNMP_NODES: '{{ contrail_analytics_snmp_nodes_string }}'
      CONFIGDB_NODES: '{{ contrail_database_nodes_string }}'
      CONFIG_NODES: '{{ contrail_config_nodes_string }}'
      CONTROL_NODES: '{{ contrail_control_nodes_string }}'
      DNS_NODES: '{{ contrail_control_nodes_string }}'
      KAFKA_NODES: '{{ contrail_kafka_service_nodes_string }}'
      RABBITMQ_NODES: '{{ contrail_database_nodes_string }}'
      RABBITMQ_NODE_PORT: 5673
  - name: role specific contrail_settings
    set_fact:
      contrail_settings:
        BGP_ASN: 64520
        BGP_AUTO_MESH: true
        HEAP_NEWSIZE: 2G
        MAX_HEAP_SIZE: 16G
        VROUTER_ENCRYPTION: false
        VROUTER_GATEWAY: 10.1.110.1
  - name: get dmi
    register: dmi
    shell: '#!/bin/sh

      dmidecode --s system-uuid | awk ''match($0, /[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}/)
      { print substr($0, RSTART, RLENGTH) }''

      '
  - name: set dmi fact
    set_fact:
      dmi_uuid: '{{ dmi.stdout }}'
  - name: get node specific contrail_settings
    register: contrail_settings_string
    shell: hiera -c /etc/puppet/hiera.yaml contrail_settings ::uuid={{ dmi_uuid }}
      | sed 's/=>/:/g'
  - block:
    - set_fact:
        contrail_settings_json: '{{ contrail_settings_string.stdout | from_json }}'
    - set_fact:
        contrail_settings: '{{ contrail_settings | combine({item.key: item.value})
          }}'
      with_dict: '{{ contrail_settings_json }}'
    name: merge node and role specific contrail_settings
    when:
    - contrail_settings_string.stdout is defined
    - contrail_settings_string.stdout != "nil"
  - lineinfile:
      dest: /etc/contrail/common_contrail.env
      line: '{{ item.key }}={{ item.value }}'
      regexp: ^{{ item.key }}.*
      state: present
    name: write contrail settings to file
    when: contrail_settings is defined
    with_dict: '{{ contrail_settings }}'
  - block:
    - blockinfile:
        block: "module contrail_nodemgr 1.0;\n\nrequire {\n        type unconfined_t;\n\
          \        type container_share_t;\n        class file entrypoint;\n}\n\n\
          #============= unconfined_t ==============\n##!!!! The file '/usr/bin/conmon'\
          \ is mislabeled on your system.\n##!!!! Fix with $ restorecon -R -v /usr/bin/conmon\n\
          allow unconfined_t container_share_t:file entrypoint;\n"
        create: true
        path: /tmp/contrail_nodemgr.te
      name: create policy file for nodemgr
    - name: create contrail nodemgr selinux policy module
      shell: /bin/checkmodule -M -m -o /tmp/contrail_nodemgr.mod /tmp/contrail_nodemgr.te
    - name: create contrail nodemgr selinux policy package
      shell: /bin/semodule_package -o /tmp/contrail_nodemgr.pp -m /tmp/contrail_nodemgr.mod
    - name: install contrail nodemgr selinux policy package
      shell: /sbin/semodule -i /tmp/contrail_nodemgr.pp
    name: create selinux policy file for nodemgr
  - name: contrail_selfsigned_ca_flag
    set_fact:
      contrail_selfsigned_ca_flag: 'true'
  - block:
    - name: contrail cert file
      set_fact:
        contrail_cert_file: ''
    - name: contrail key file
      set_fact:
        contrail_certkey_file: ''
    - name: remove current cert files
      shell: rm -f {{ contrail_cert_file }} {{ contrail_certkey_file }}
    - name: re-run node-init container
      shell: "if {{ container_cli }} inspect contrail-node-init >/dev/null 2>&1 ;\
        \ then\n  {{ container_cli }} start -i contrail-node-init\nfi\n"
    name: Re-create Contrail self-signed certs
    when: '{{ contrail_selfsigned_ca_flag | bool }}'
  name: Contrail Base host prep tasks
- block:
  - blockinfile:
      block: '(block contrail_container

        (allow chkpwd_t container_file_t (lnk_file (read)))

        (allow container_t cloud_init_t ( unix_dgram_socket ( sendto )))

        (allow container_t container_file_t (chr_file (read)))

        (allow container_t container_share_t ( file ( entrypoint )))

        (allow container_t container_var_run_t (file ( openat open read write lock
        )))

        (allow container_t mount_var_run_t (dir ( openat read write )))

        (allow container_t spc_t (dir (getattr)))

        (allow container_t spc_t (fifo_file (write ioctl getattr setattr)))

        (allow container_t system_dbusd_t (dbus (send_msg)))

        (allow container_t var_t ( dir ( openat open read getattr lock search ioctl
        add_name remove_name write setattr create )))

        (allow container_t var_t ( file ( getattr read write append ioctl lock map
        open create setattr unlink )))

        (allow container_t var_t ( sock_file ( getattr read write append open )))

        (allow container_t var_lib_t ( dir ( openat open read getattr lock search
        ioctl add_name remove_name write setattr create )))

        (allow container_t var_lib_t ( file ( getattr read write append ioctl lock
        map open create setattr unlink )))

        (allow container_t var_lib_t ( sock_file ( getattr read write append open
        )))

        (allow chronyc_t container_file_t (lnk_file ( read )))

        (allow chronyd_t container_file_t (lnk_file ( read )))

        (allow chronyd_t container_file_t ( file ( open getattr read)))

        (allow chronyd_t spc_t ( unix_dgram_socket ( sendto )))

        (allow chronyd_t cloud_init_t ( unix_dgram_socket ( sendto )))

        (allow certmonger_t sudo_exec_t ( file ( execute execute_no_trans open read
        )))

        (allow certmonger_t container_file_t ( dir ( search open read getattr setattr
        add_name remove_name create )))

        (allow certmonger_t container_file_t ( file ( getattr read write append ioctl
        lock map open create setattr unlink )))

        (allow certmonger_t container_file_t ( lnk_file ( read )))

        (allow certmonger_t self ( capability ( sys_resource )))

        (allow certmonger_t self ( netlink_audit_socket ( create )))

        (allow certmonger_t sssd_conf_t ( dir ( search openat read write )))

        (allow certmonger_t sssd_conf_t ( file ( getattr read open )))

        (allow certmonger_t chkpwd_exec_t ( file ( execute )))

        (allow certmonger_t self  (process ( setrlimit )))

        (allow dhcpc_t container_file_t (file ( open getattr setattr read write)))

        (allow kmod_t container_file_t (system ( module_load )))

        (allow named_t container_file_t (file (open getattr read)))

        (allow NetworkManager_t container_file_t (file (read open)))

        (allow rhsmcertd_t container_file_t (file (open getattr read)))

        (allow setroubleshootd_t container_file_t (file (open getattr read)))

        (allow snmpd_t container_file_t (file (open read getattr)))

        (allow sshd_t container_file_t (lnk_file (read)))

        (allow sssd_t container_file_t (file (open getattr read)))

        (allow system_dbusd_t container_file_t (lnk_file (read)))

        (allow systemd_logind_t container_file_t (lnk_file (read)))

        (allow svirt_tcg_t container_file_t ( dir ( read  )))

        (allow timedatex_t container_file_t ( lnk_file ( read )))

        (allow systemd_sysctl_t container_file_t ( file ( getattr open read )))

        )

        '
      create: true
      marker: ; {mark} ANSIBLE MANAGED BLOCK
      path: /tmp/contrail_container.cil
    name: create selinux module file /tmp/contrail_container.cil
  - name: create selinux module from file /tmp/contrail_container.cil
    shell: /sbin/semodule -i /tmp/contrail_container.cil
  name: create selinux module for unprivileged contrail containers
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent logs directory
  with_items:
  - mode: '0750'
    path: /var/log/containers/glance
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/httpd/glance
    setype: svirt_sandbox_file_t
- mount: name=/var/lib/glance/images src="{{nfs_share}}" fstype=nfs opts="{{nfs_options}}"
    state=mounted
  name: Mount NFS on host
  vars:
    glance_netapp_nfs_enabled: false
    glance_nfs_share: ''
    netapp_share_location: ''
    nfs_backend_enabled: false
    nfs_options: _netdev,bg,intr,context=system_u:object_r:svirt_sandbox_file_t:s0
    nfs_share: '{{ glance_nfs_share if (glance_nfs_share) else netapp_share_location
      }}'
  when: nfs_backend_enabled or glance_netapp_nfs_enabled
- mount: name="{{glance_node_staging_uri[7:]}}" src="{{glance_staging_nfs_share}}"
    fstype=nfs opts="{{glance_nfs_options}}" state=mounted
  name: Mount Node Staging Location
  vars:
    glance_nfs_options: _netdev,bg,intr,context=system_u:object_r:svirt_sandbox_file_t:s0
    glance_node_staging_uri: file:///var/lib/glance/staging
    glance_staging_nfs_share: ''
  when: glance_staging_nfs_share != ''
- file:
    path: /var/lib/glance
    setype: svirt_sandbox_file_t
    state: directory
  name: ensure /var/lib/glance exists
- name: get parameters
  no_log: true
  set_fact:
    cert_content: ''
    cert_path: /etc/pki/tls/private/overcloud_endpoint.pem
    chain_content: ''
    key_content: ''
- block:
  - name: get DeployedSSLCertificatePath attributes
    register: attr_cert_path
    stat:
      path: '{{cert_path}}'
  - name: set is_haproxy_bootstrap_node fact
    set_fact: is_haproxy_bootstrap_node={{haproxy_short_bootstrap_node_name | lower
      == ansible_facts['hostname'] | lower}}
    when:
    - haproxy_short_bootstrap_node_name|default(false)
  - name: get haproxy status
    register: haproxy_state
    systemd:
      name: haproxy
  - name: get pacemaker status
    register: pacemaker_state
    systemd:
      name: pacemaker
  - name: get docker status
    register: docker_state
    systemd:
      name: docker
  - command: '{{ container_cli }} ps -q -f name=haproxy'
    name: get container_id
    register: container_id
    when:
    - docker_state.status.ActiveState == 'active' or container_cli == 'podman'
    - attr_cert_path.stat.exists
    - attr_cert_path.stat.isdir == False
  - name: get pcs resource name for haproxy container
    register: pacemaker_resource
    shell: 'pcs status resources | sed -n ''s/^.*container.*: \(haproxy.*\) .*/\1/p''

      '
    when:
    - bootstrap_node is defined
    - is_haproxy_bootstrap_node
    - pacemaker_state.status.ActiveState == 'active'
    - attr_cert_path.stat.exists
    - attr_cert_path.stat.isdir
  - file:
      path: '{{cert_path}}'
      state: absent
    name: remove DeployedSSLCertificatePath if is dir
    when: attr_cert_path.stat.isdir is defined and attr_cert_path.stat.isdir
  - copy:
      content: '{{cert_content}}

        {{chain_content}}

        {{key_content}}

        '
      dest: '{{cert_path}}'
      mode: 288
      owner: root
    name: push certificate content
    no_log: true
  - block:
    - file:
        group: haproxy
        path: '{{cert_path}}'
      name: set certificate ownership
    - name: reload haproxy if enabled
      service:
        name: haproxy
        state: reloaded
    name: BM haproxy non-pacemaker context
    when: haproxy_state.status.ActiveState == 'active'
  - command: pcs resource restart "{{pacemaker_resource.stdout}}"
    name: restart pacemaker resource for haproxy
    when:
    - pacemaker_resource is defined
    - pacemaker_resource.stdout is defined
    - pacemaker_resource.stdout != ''
  - block:
    - name: copy certificate, chgrp, restart haproxy
      shell: "set -e\nif {{ container_cli }} ps -f \"id={{ item }}\" --format \"{{\
        \ '{{' }}.Names{{ '}}' }}\" | grep -q \"^haproxy-bundle\"; then\n  tar -c\
        \ {{ cert_path }} | {{container_cli}} exec -i {{ item }} tar -C / -xv\nelse\n\
        \  {{ container_cli }} cp {{ cert_path }} {{ item }}:{{ cert_path }}\nfi\n\
        {{ container_cli }} exec --user root {{ item }} chgrp haproxy {{ cert_path\
        \ }}\n{{ container_cli }} kill --signal=HUP {{ item }}\n"
      with_items: '{{ container_id.stdout.split(''

        '') }}'
    name: dedicated part for containers
    when:
    - container_id is defined
    - container_id.stdout is defined
    - container_id.stdout != ''
  name: manage certificate
  when:
  - cert_content is defined
  - cert_content != ''
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/haproxy
    setype: var_log_t
  - path: /var/lib/haproxy
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/haproxy
    setype: var_log_t
  - path: /var/lib/haproxy
    setype: svirt_sandbox_file_t
  - path: /var/log/haproxy
    setype: svirt_sandbox_file_t
- name: Run puppet on the host to apply IPtables rules
  shell: "set +e\npuppet apply {{ puppet_debug }} --detailed-exitcodes --summarize\
    \ --color=false \\\n    --modulepath '{{ puppet_modulepath }}' --tags '{{ puppet_tags\
    \ }}' -e '{{ puppet_execute }}'\nrc=$?\nset -e\nset +ux\nif [ $rc -eq 2 -o $rc\
    \ -eq 0 ]; then\n    exit 0\nfi\nexit $rc\n"
  vars:
    puppet_debug: ''
    puppet_execute: 'if hiera(''enable_load_balancer'', true) { class {''::tripleo::haproxy'':
      use_internal_certificates => false, manage_firewall => hiera(''tripleo::firewall::manage_firewall'',
      true), }}'
    puppet_modulepath: /etc/puppet/modules:/opt/stack/puppet-modules:/usr/share/openstack-puppet/modules
    puppet_tags: tripleo::firewall::rule
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/heat
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/httpd/heat-api
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/heat
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/httpd/heat-api-cfn
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/heat
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/horizon
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/httpd/horizon
    setype: svirt_sandbox_file_t
  - path: /var/www
    setype: svirt_sandbox_file_t
  - mode: '1777'
    path: /var/tmp/horizon
    setype: svirt_sandbox_file_t
- copy:
    content: 'd /var/tmp/horizon 1777 root root - -

      '
    dest: /etc/tmpfiles.d/var-tmp-horizon.conf
  name: ensure /var/tmp/horizon exists on boot
- file:
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - path: /etc/iscsi
    setype: svirt_sandbox_file_t
  - path: /etc/target
    setype: svirt_sandbox_file_t
  - path: /var/lib/iscsi
    setype: svirt_sandbox_file_t
- name: stat /lib/systemd/system/iscsid.socket
  register: stat_iscsid_socket
  stat: path=/lib/systemd/system/iscsid.socket
- name: Stop and disable iscsid.socket service
  service: name=iscsid.socket state=stopped enabled=no
  when: stat_iscsid_socket.stat.exists
- command: systemctl is-enabled --quiet iscsi.service
  failed_when: false
  name: Check if iscsi.service is enabled
  register: iscsi_service_enabled_result
- name: Stop iscsi.service
  service: name=iscsi.service state=stopped enabled=no
  when:
  - iscsi_service_enabled_result is changed
  - iscsi_service_enabled_result.rc == 0
- include_role:
    name: tripleo-kernel
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/keystone
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/httpd/keystone
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/memcached
    setype: container_file_t
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent logs directory
  with_items:
  - mode: '0750'
    path: /var/log/containers/metrics_qdr
    setype: svirt_sandbox_file_t
  - path: /var/lib/metrics_qdr
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/mysql
    setype: svirt_sandbox_file_t
  - path: /var/lib/mysql
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/mariadb
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/neutron
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/httpd/neutron-api
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/nova
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/httpd/nova-api
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/nova
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - mode: '0750'
    path: /var/log/containers/nova
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/httpd/nova-metadata
    setype: svirt_sandbox_file_t
- group:
    gid: 107
    name: qemu
    state: present
  name: ensure qemu group is present on the host
- name: ensure qemu user is present on the host
  user:
    comment: qemu user
    group: qemu
    name: qemu
    shell: /sbin/nologin
    state: present
    uid: 107
- name: allow logrotate to read inside containers
  seboolean:
    name: logrotate_read_inside_containers
    persistent: true
    state: true
- file:
    mode: '{{ item.mode }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent logs directory
  with_items:
  - mode: '0750'
    path: /var/log/containers/placement
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/httpd/placement
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - path: /var/lib/rabbitmq
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/rabbitmq
    setype: svirt_sandbox_file_t
- name: stop the Erlang port mapper on the host and make sure it cannot bind to the
    port used by container
  shell: 'echo ''export ERL_EPMD_ADDRESS=127.0.0.1'' > /etc/rabbitmq/rabbitmq-env.conf

    echo ''export ERL_EPMD_PORT=4370'' >> /etc/rabbitmq/rabbitmq-env.conf

    for pid in $(pgrep epmd --ns 1 --nslist pid); do kill $pid; done

    '
- block:
  - name: Set login facts
    set_fact:
      container_default_pids_limit: 4096
      container_events_logger_mechanism: journald
      container_registry_insecure_registries:
      - satellite.mgmt.tmeprj2.comm.red.ibm.gsc:5000
      container_registry_login: true
      container_registry_logins: {}
      container_registry_logins_json: {}
  - name: Convert logins json to dict
    set_fact:
      container_registry_logins: '{{ container_registry_logins_json | from_json }}'
    when:
    - container_registry_logins_json is string
    - container_registry_login | bool
    - (container_registry_logins_json | length) > 0
  - name: Set registry logins
    set_fact:
      container_registry_logins: '{{ container_registry_logins_json }}'
    when:
    - container_registry_logins_json is mapping
    - container_registry_login | bool
    - (container_registry_logins_json | length) > 0
  - include_role:
      name: tripleo-podman
      tasks_from: tripleo_podman_install.yml
    name: Run podman install
    vars:
      tripleo_container_default_pids_limit: '{{ container_default_pids_limit }}'
      tripleo_container_events_logger_mechanism: '{{ container_events_logger_mechanism
        }}'
      tripleo_container_registry_insecure_registries: '{{ container_registry_insecure_registries
        }}'
  - include_role:
      name: tripleo-podman
      tasks_from: tripleo_podman_login.yml
    name: Run podman login
    vars:
      tripleo_container_registry_login: '{{ container_registry_login | bool }}'
      tripleo_container_registry_logins: '{{ container_registry_logins }}'
  name: Install and configure Podman
- copy:
    content: 'This file makes paunch generate additional systemd

      dependencies for containers that have special

      start/stop ordering constraints. It ensures that

      those constraints are enforced on reboot/shutdown.

      '
    dest: /etc/sysconfig/podman_drop_in
  name: Configure paunch to generate systemd drop-in dependencies
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - path: /var/lib/redis
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/redis
    setype: svirt_sandbox_file_t
  - path: /var/run/redis
    setype: svirt_sandbox_file_t
- copy:
    content: 'd /var/run/redis 0755 root root - -

      '
    dest: /etc/tmpfiles.d/var-run-redis.conf
  name: ensure /var/run/redis is present upon reboot
- import_role:
    name: redhat-subscription
  name: Red Hat Subscription Management configuration during deployment
  vars:
    rhsm_activation_key: osp16-2-dev-overcloud
    rhsm_baseurl: https://satellite.mgmt.tmeprj2.comm.red.ibm.gsc/pulp/repos
    rhsm_force_register: true
    rhsm_method: satellite
    rhsm_org_id: default
    rhsm_release: 8.4
    rhsm_server_hostname: satellite.mgmt.tmeprj2.comm.red.ibm.gsc
- file:
    mode: '0750'
    path: /var/log/containers/rsyslog
    setype: svirt_sandbox_file_t
    state: directory
  name: create persistent logs directory for rsyslog
- file:
    path: /var/lib/rsyslog.container
    setype: svirt_sandbox_file_t
    state: directory
  name: create persistent state directory for rsyslog
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - path: /srv/node
    setype: svirt_sandbox_file_t
  - path: /var/log/swift
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/swift
    setype: svirt_sandbox_file_t
- file:
    mode: '{{ item.mode|default(omit) }}'
    path: '{{ item.path }}'
    setype: '{{ item.setype }}'
    state: directory
  name: create persistent directories
  with_items:
  - path: /srv/node
    setype: svirt_sandbox_file_t
  - path: /var/cache/swift
    setype: svirt_sandbox_file_t
  - mode: '0750'
    path: /var/log/containers/swift
    setype: svirt_sandbox_file_t
- name: Set swift_use_local_disks fact
  set_fact:
    swift_use_local_disks: true
- args:
    executable: /bin/bash
  changed_when: _move_dir.rc == 2
  failed_when: _move_dir.rc not in [0,2]
  name: Move deprecated UC Swift storage directory if it exists
  register: _move_dir
  shell: "set -o pipefail\nEXIT_CODE=0\nif [ -d /srv/node/1 ]; then\n  mv /srv/node/1\
    \ /srv/node/d1\n  EXIT_CODE=2\nfi\nexit ${EXIT_CODE}\n"
- file:
    path: /srv/node/d1
    state: directory
  name: Create Swift d1 directory if needed
  when: swift_use_local_disks
- name: Set fact for SwiftRawDisks
  set_fact:
    swift_raw_disks: {}
- filesystem:
    dev: '{{ swift_raw_disks[item][''base_dir'']|default(''/dev'') }}/{{ item }}'
    fstype: xfs
    opts: -f -i size=1024
  name: Format SwiftRawDisks
  when: swift_raw_disks
  with_items: '{{ swift_raw_disks }}'
- name: Refresh facts if SwiftRawDisks is set to get uuids if newly created partitions
  setup:
    filter: ansible_device_links
    gather_subset:
    - '!all'
    - hardware
  when: swift_raw_disks
- mount:
    fstype: xfs
    name: /srv/node/{{ item }}
    opts: noatime
    src: '{% if lsblk.results[''uuids''][item] is defined %}UUID={{ ansible_facts[''device_links''][''uuids''][item][0]
      }}{% else %}{{ swift_raw_disks[item][''base_dir'']|default(''/dev'') }}/{{ item
      }}{% endif %}'
    state: mounted
  name: Mount devices defined in SwiftRawDisks
  when: swift_raw_disks
  with_items: '{{ swift_raw_disks }}'
- become: true
  failed_when: false
  name: Check for NTP service
  register: ntp_service_check
  shell: systemctl is-active ntpd.service || systemctl is-enabled ntpd.service
- name: Disable NTP before configuring Chrony
  service:
    enabled: false
    name: ntpd
    state: stopped
  when:
  - ntp_service_check.rc is defined
  - ntp_service_check.rc == 0
- include_role:
    name: chrony
  name: Install, Configure and Run Chrony
- meta: flush_handlers
  name: Ensure chrony has been restarted
- command: chronyc makestep
  name: Ensure system is NTP time synced
- name: Set timezone fact
  set_fact:
    timezone: UTC
- name: Set timezone to {{ timezone | default('UTC') }}
  register: timezone_result
  timezone:
    name: '{{ timezone }}'
- failed_when: false
  name: Restart services
  service:
    name: '{{ item }}'
    state: restarted
  when:
  - timezone_result.changed
  with_items:
  - rsyslog
  - crond
- include_role:
    name: tuned
